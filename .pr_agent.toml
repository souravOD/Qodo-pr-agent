[config]
# Use LiteLLM proxy-compatible OpenAI models with explicit provider prefix.
model = "openai/gpt-5"
fallback_models = ["openai/gpt-5-mini", "openai/gpt-5-nano"]

[openai]
# Route all OpenAI-style calls through the LiteLLM proxy.
api_base = "https://litellm.confer.today"

[pr_reviewer]
enable_review_labels_effort = true
enable_auto_approval = true
